{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello\n",
    "\n",
    "This notebook is inspired from \"Lightgbm(Regressor) | Kaggle\" kernel.\n",
    "https://www.kaggle.com/hiro5299834/wifi-features-with-lightgbm-kfold\n",
    "\n",
    "\n",
    "I am studying Machine Learning and I was requested to participate to one competition of my choise on Kaggle.\n",
    "So I am doing it as an exercice.\n",
    "Please be indulgent.\n",
    "My plan here is to capture all the big steps in the process of machine learning modeling and to test some specific models that are not yet used by other kernels on this competition.\n",
    "\n",
    "My first thought is to test Xgboost since the primary Kernel I am inspiring on is using Lightgbm model.\n",
    "\n",
    "I will test another model later on.\n",
    "\n",
    "For the exploratory step I have been inspired by these notebooks:\n",
    "- https://www.kaggle.com/chandrylpaternetony/data-descript-outlier-detect-floor-mapping\n",
    "\n",
    "- https://www.kaggle.com/ihelon/indoor-location-exploratory-data-analysis/notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick overview of the input data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sites 204\n",
      "Number of train meta 204\n",
      "Number of test sites 626\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Number of train sites {len(os.listdir('../input/indoor-location-navigation/train/'))}\")\n",
    "print(f\"Number of train meta {len(os.listdir('../input/indoor-location-navigation/metadata/'))}\")\n",
    "print(f\"Number of test sites {len(os.listdir('../input/indoor-location-navigation/test/'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26925\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_data_structure = list()\n",
    "for site in os.listdir('../input/indoor-location-navigation/train/'):\n",
    "    for floor in os.listdir('../input/indoor-location-navigation/train/' + site + '/'):\n",
    "        for phone in os.listdir('../input/indoor-location-navigation/train/' + site + '/' + floor + '/'):\n",
    "            input_data_structure.append([site,floor,phone])\n",
    "print(len(input_data_structure))            \n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion into pandas DataFrame for quick statistics over the input data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sites</th>\n",
       "      <th>floor</th>\n",
       "      <th>phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26925</td>\n",
       "      <td>26925</td>\n",
       "      <td>26925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>204</td>\n",
       "      <td>43</td>\n",
       "      <td>26925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>5d27075f03f801723c2e360f</td>\n",
       "      <td>F1</td>\n",
       "      <td>5d60ad2404ffc90008edf43d.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1141</td>\n",
       "      <td>4557</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           sites  floor                         phone\n",
       "count                      26925  26925                         26925\n",
       "unique                       204     43                         26925\n",
       "top     5d27075f03f801723c2e360f     F1  5d60ad2404ffc90008edf43d.txt\n",
       "freq                        1141   4557                             1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_df = pd.DataFrame(input_data_structure, columns=['sites','floor','phone'])\n",
    "input_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are **204** different **sites**\n",
    "* There are **43!!!** different **floors** identification, be aware F1 or 1F is the same floor. So we obviously have less then 43 different floors\n",
    "* there are **26925** different **phones** and every phone has only one single recording and has been recording at the same floor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                phone\n",
      "sites                    floor       \n",
      "5a0546857ecc773753327266 B1       109\n",
      "                         F1       131\n",
      "                         F2       110\n",
      "                         F3        78\n",
      "                         F4        86\n",
      "...                               ...\n",
      "5dc8cea7659e181adb076a3f F4        79\n",
      "                         F5       103\n",
      "                         F6        81\n",
      "                         F7        40\n",
      "All                             26925\n",
      "\n",
      "[982 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "table = pd.pivot_table(input_df, values=['phone'], index=['sites','floor'], margins=True,\n",
    "                    aggfunc={\"phone\": \"count\"})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>982.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.837067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>859.103814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>26925.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              phone\n",
       "count    982.000000\n",
       "mean      54.837067\n",
       "std      859.103814\n",
       "min        1.000000\n",
       "25%        6.000000\n",
       "50%       14.000000\n",
       "75%       31.000000\n",
       "max    26925.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_floor = pd.pivot_table(input_df, values=['floor'], index=['sites'], margins=True,\n",
    "                    aggfunc={\"floor\": \"count\"})\n",
    "print(table_floor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(table_floor.index[:-1],table_floor['floor'][:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on Wifi features.\n",
    "We will take for granted the preprocessing of input data **indoor-location-navigation** and its transformation into **indoor-navigation-and-location-wifi-features**\n",
    "\n",
    "About this Dataset\n",
    "Content\n",
    "Version2 update:\n",
    "Contains features for the indoor location and navigation competition. They were based on \"Indoor Navigation and Location Wifi Features\" data by @devinanzelmo and generated using only wifi's bssid in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'ntpath' from 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\ntpath.py'>\n",
      "Number of csv inputs 48\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "local_path = 'C:/Users/hp/Documents/OpenClassrooms/P8/'\n",
    "# print(os.path)\n",
    "# print(f\"Number of csv inputs {len(os.listdir('../input/indoor-navigation-and-location-wifi-features/'))}\")\n",
    "print(f\"Number of csv inputs {len(os.listdir(local_path + 'input/indoor-navigation-and-location-wifi-features/'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_params = {'max_depth':2, 'eta':1, 'objective':'reg:squarederror', 'eval_metric':'rmse', 'seed': SEED }\n",
    "\n",
    "# using scikit learn API\n",
    "\n",
    "''' lgb_params = {'objective': 'root_mean_squared_error',\n",
    "              'boosting_type': 'gbdt',\n",
    "              'n_estimators': 50000,\n",
    "              'learning_rate': 0.1,\n",
    "              'num_leaves': 90,\n",
    "              'colsample_bytree': 0.4,\n",
    "              'subsample': 0.6,\n",
    "              'subsample_freq': 2,\n",
    "              'bagging_seed': SEED,\n",
    "              'reg_alpha': 8,\n",
    "              'reg_lambda': 2,\n",
    "              'random_state': SEED,\n",
    "              'n_jobs': -1\n",
    "              }\n",
    "'''\n",
    "xgb_params = {'objective': 'root_mean_squared_error',\n",
    "              'booster': 'gbtree',\n",
    "              'n_estimators': 50000,\n",
    "              'learning_rate': 0.1,              \n",
    "              'reg_alpha': 8,\n",
    "              'reg_lambda': 2,\n",
    "              'random_state': SEED,\n",
    "              'n_jobs': -1\n",
    "              }\n",
    "\n",
    "# classifier\n",
    "'''\n",
    "xgb_f_params = {'objective': 'multiclass',\n",
    "                'booster': 'gbtree',\n",
    "                'n_estimators': 50000,\n",
    "                'learning_rate': 0.1,                \n",
    "                'reg_alpha': 10,\n",
    "                'reg_lambda': 2,\n",
    "                'random_state': SEED,\n",
    "                'n_jobs': -1\n",
    "                }\n",
    "                '''\n",
    "\n",
    "Parameters\n",
    "n_estimators (int) – Number of boosting rounds.\n",
    "\n",
    "use_label_encoder (bool) – (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new code, we recommend that you set this parameter to False.\n",
    "\n",
    "max_depth (int) – Maximum tree depth for base learners.\n",
    "\n",
    "learning_rate (float) – Boosting learning rate (xgb’s “eta”)\n",
    "\n",
    "verbosity (int) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "\n",
    "objective (string or callable) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n",
    "\n",
    "booster (string) – Specify which booster to use: gbtree, gblinear or dart.\n",
    "\n",
    "tree_method (string) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from parameters document.\n",
    "\n",
    "n_jobs (int) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n",
    "\n",
    "gamma (float) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "\n",
    "min_child_weight (float) – Minimum sum of instance weight(hessian) needed in a child.\n",
    "\n",
    "max_delta_step (float) – Maximum delta step we allow each tree’s weight estimation to be.\n",
    "\n",
    "subsample (float) – Subsample ratio of the training instance.\n",
    "\n",
    "colsample_bytree (float) – Subsample ratio of columns when constructing each tree.\n",
    "\n",
    "colsample_bylevel (float) – Subsample ratio of columns for each level.\n",
    "\n",
    "colsample_bynode (float) – Subsample ratio of columns for each split.\n",
    "\n",
    "reg_alpha (float (xgb's alpha)) – L1 regularization term on weights\n",
    "\n",
    "reg_lambda (float (xgb's lambda)) – L2 regularization term on weights\n",
    "\n",
    "scale_pos_weight (float) – Balancing of positive and negative weights.\n",
    "\n",
    "base_score – The initial prediction score of all instances, global bias.\n",
    "\n",
    "random_state (int) –\n",
    "\n",
    "Random number seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "subm = pd.read_csv('../input/indoor-location-navigation/sample_submission.csv', index_col=0)\n",
    "subm.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10133 entries, 5a0546857ecc773753327266_046cfa46be49fc10834815c6_0000000000009 to 5dc8cea7659e181adb076a3f_fd64de8c4a2fc5ebb0e9f412_0000000100447\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   floor   10133 non-null  int64  \n",
      " 1   x       10133 non-null  float64\n",
      " 2   y       10133 non-null  float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 316.7+ KB\n"
     ]
    }
   ],
   "source": [
    "subm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "../input/indoor-navigation-and-location-wifi-features/5a0546857ecc773753327266_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9012 entries, 4 to 4\n",
      "Columns: 1216 entries, 00505a2c465e4a7f52e64beb1902c9aee8b04a90 to path\n",
      "dtypes: float64(2), int64(1213), object(1)\n",
      "memory usage: 83.7+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv('../input/indoor-navigation-and-location-wifi-features/5a0546857ecc773753327266_test.csv',index_col=0)\n",
    "# df = pd.read_csv('../input/indoor-navigation-and-location-wifi-features/5a0546857ecc773753327266_train.csv',index_col=0)\n",
    "df = pd.read_csv('../input/indoor-navigation-and-location-wifi-features/5da138314db8ce0c98bbf3a0_train.csv',index_col=0)\n",
    "# df = pd.read_csv('../input/indoor-navigation-and-location-wifi-features/5d2709a003f801723c3251bf_train.csv',index_col=0)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "4921258f-5532-44da-b9da-4e920592a368",
    "_uuid": "96bb8a60-72bc-4b0b-8798-795dfda98988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24 *_train.csv files\n",
      "There are 24 *_test.csv files\n",
      "process training set for site file num 0\n",
      "process testing set for site file num 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.1GB(-0.4GB): 607.898sec] fit X\n",
      "[0.3GB(+0.2GB): 606.982sec] fit Y\n",
      "[0.3GB(+0.0GB): 271.742sec] fit F\n",
      "[0.3GB(+0.0GB): 0.584sec] Pred X\n",
      "[0.3GB(+0.0GB): 0.220sec] Pred Y\n",
      "[0.3GB(+0.0GB): 0.240sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0: mean position error 3.72969839558177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 602.802sec] fit X\n",
      "[0.5GB(+0.0GB): 601.672sec] fit Y\n",
      "[0.4GB(-0.1GB): 284.680sec] fit F\n",
      "[0.4GB(+0.0GB): 0.251sec] Pred X\n",
      "[0.4GB(+0.0GB): 0.254sec] Pred Y\n",
      "[0.4GB(+0.0GB): 0.232sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1: mean position error 3.7192771108315843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 604.261sec] fit X\n",
      "[0.5GB(+0.0GB): 591.295sec] fit Y\n",
      "[0.5GB(+0.0GB): 275.021sec] fit F\n",
      "[0.5GB(+0.0GB): 0.223sec] Pred X\n",
      "[0.5GB(+0.0GB): 0.247sec] Pred Y\n",
      "[0.5GB(+0.0GB): 0.258sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2: mean position error 3.7960548511129155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(-0.1GB): 11408.186sec] fit X\n",
      "[0.3GB(-0.2GB): 20239.205sec] fit Y\n",
      "[0.3GB(+0.0GB): 283.740sec] fit F\n",
      "[0.3GB(+0.0GB): 0.894sec] Pred X\n",
      "[0.3GB(+0.0GB): 0.226sec] Pred Y\n",
      "[0.3GB(+0.0GB): 0.216sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3: mean position error 3.8568158782328434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 603.939sec] fit X\n",
      "[0.5GB(+0.0GB): 2031.430sec] fit Y\n",
      "[0.3GB(-0.2GB): 280.841sec] fit F\n",
      "[0.3GB(+0.0GB): 0.244sec] Pred X\n",
      "[0.3GB(+0.0GB): 0.238sec] Pred Y\n",
      "[0.3GB(+0.0GB): 0.231sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4: mean position error 3.9860742055553327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(-0.0GB): 599.377sec] fit X\n",
      "[0.4GB(-0.1GB): 603.370sec] fit Y\n",
      "[0.4GB(-0.0GB): 266.053sec] fit F\n",
      "[0.4GB(+0.0GB): 0.262sec] Pred X\n",
      "[0.4GB(+0.0GB): 0.292sec] Pred Y\n",
      "[0.4GB(+0.0GB): 0.267sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5: mean position error 3.6562433949970097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(-0.0GB): 594.653sec] fit X\n",
      "[0.5GB(+0.0GB): 610.095sec] fit Y\n",
      "[0.4GB(-0.1GB): 280.982sec] fit F\n",
      "[0.4GB(+0.0GB): 0.199sec] Pred X\n",
      "[0.4GB(+0.0GB): 0.191sec] Pred Y\n",
      "[0.4GB(+0.0GB): 0.211sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 6: mean position error 3.637754110788491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 603.207sec] fit X\n",
      "[0.4GB(-0.1GB): 601.590sec] fit Y\n",
      "[0.4GB(+0.0GB): 267.987sec] fit F\n",
      "[0.4GB(+0.0GB): 0.221sec] Pred X\n",
      "[0.4GB(+0.0GB): 0.211sec] Pred Y\n",
      "[0.4GB(+0.0GB): 0.233sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 7: mean position error 3.5693114487742656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(-0.1GB): 598.822sec] fit X\n",
      "[0.4GB(-0.0GB): 609.178sec] fit Y\n",
      "[0.4GB(+0.0GB): 261.822sec] fit F\n",
      "[0.4GB(+0.0GB): 0.211sec] Pred X\n",
      "[0.4GB(+0.0GB): 0.209sec] Pred Y\n",
      "[0.4GB(+0.0GB): 0.245sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 8: mean position error 3.663430646698036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 593.262sec] fit X\n",
      "[0.5GB(+0.0GB): 596.385sec] fit Y\n",
      "[0.5GB(+0.0GB): 283.026sec] fit F\n",
      "[0.5GB(+0.0GB): 0.213sec] Pred X\n",
      "[0.5GB(+0.0GB): 0.220sec] Pred Y\n",
      "[0.5GB(+0.0GB): 0.224sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 9: mean position error 3.6502306603600108\n",
      "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+\n",
      "file #0, shape=(9296, 3401), name=5a0546857ecc773753327266_train.csv\n",
      "mean position error 3.7265305106347224\n",
      "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+\n",
      "process training set for site file num 1\n",
      "process testing set for site file num 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(-0.2GB): 571.543sec] fit X\n",
      "[0.3GB(-0.2GB): 556.362sec] fit Y\n",
      "[0.3GB(+0.0GB): 288.460sec] fit F\n",
      "[0.3GB(+0.0GB): 0.216sec] Pred X\n",
      "[0.3GB(+0.0GB): 0.116sec] Pred Y\n",
      "[0.3GB(+0.0GB): 0.135sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0: mean position error 4.556725802348552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 557.213sec] fit X\n",
      "[0.5GB(+0.0GB): 559.466sec] fit Y\n",
      "[0.5GB(+0.0GB): 287.809sec] fit F\n",
      "[0.5GB(+0.0GB): 0.120sec] Pred X\n",
      "[0.5GB(+0.0GB): 0.120sec] Pred Y\n",
      "[0.5GB(+0.0GB): 0.129sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1: mean position error 4.675078999851532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 1355.236sec] fit X\n",
      "[0.4GB(-0.1GB): 447.135sec] fit Y\n",
      "[0.4GB(-0.0GB): 297.357sec] fit F\n",
      "[0.4GB(+0.0GB): 0.130sec] Pred X\n",
      "[0.4GB(+0.0GB): 0.130sec] Pred Y\n",
      "[0.4GB(+0.0GB): 0.123sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2: mean position error 4.891041930814142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 557.371sec] fit X\n",
      "[0.5GB(+0.0GB): 558.787sec] fit Y\n",
      "[0.5GB(+0.0GB): 286.647sec] fit F\n",
      "[0.5GB(+0.0GB): 0.120sec] Pred X\n",
      "[0.5GB(+0.0GB): 0.120sec] Pred Y\n",
      "[0.5GB(+0.0GB): 0.132sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3: mean position error 4.8019781560266885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 556.515sec] fit X\n",
      "[0.5GB(+0.0GB): 570.013sec] fit Y\n",
      "[0.5GB(-0.0GB): 291.341sec] fit F\n",
      "[0.5GB(+0.0GB): 0.120sec] Pred X\n",
      "[0.5GB(+0.0GB): 0.120sec] Pred Y\n",
      "[0.5GB(+0.0GB): 0.125sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4: mean position error 4.682949335516161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 561.235sec] fit X\n",
      "[0.5GB(+0.0GB): 573.599sec] fit Y\n",
      "[0.4GB(-0.1GB): 301.208sec] fit F\n",
      "[0.4GB(+0.0GB): 0.123sec] Pred X\n",
      "[0.4GB(+0.0GB): 0.121sec] Pred Y\n",
      "[0.4GB(+0.0GB): 0.125sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5: mean position error 4.582284674309531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 559.718sec] fit X\n",
      "[0.5GB(+0.0GB): 559.113sec] fit Y\n",
      "[0.5GB(+0.0GB): 291.309sec] fit F\n",
      "[0.5GB(+0.0GB): 0.124sec] Pred X\n",
      "[0.5GB(+0.0GB): 0.122sec] Pred Y\n",
      "[0.5GB(+0.0GB): 0.127sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 6: mean position error 4.759496199480442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 820.772sec] fit X\n",
      "[0.4GB(-0.1GB): 565.506sec] fit Y\n",
      "[0.4GB(+0.0GB): 291.773sec] fit F\n",
      "[0.4GB(+0.0GB): 0.244sec] Pred X\n",
      "[0.4GB(+0.0GB): 0.122sec] Pred Y\n",
      "[0.4GB(+0.0GB): 0.124sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 7: mean position error 4.797642947548485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 561.327sec] fit X\n",
      "[0.4GB(-0.1GB): 616.288sec] fit Y\n",
      "[0.3GB(-0.1GB): 324.655sec] fit F\n",
      "[0.3GB(+0.0GB): 1.933sec] Pred X\n",
      "[0.3GB(+0.0GB): 0.128sec] Pred Y\n",
      "[0.3GB(+0.0GB): 0.141sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 8: mean position error 4.77142754632669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.5GB(+0.0GB): 582.142sec] fit X\n",
      "[0.4GB(-0.1GB): 563.902sec] fit Y\n",
      "[0.4GB(-0.0GB): 288.926sec] fit F\n",
      "[0.4GB(+0.0GB): 0.382sec] Pred X\n",
      "[0.4GB(+0.0GB): 0.119sec] Pred Y\n",
      "[0.4GB(+0.0GB): 0.124sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 9: mean position error 4.6598225591795535\n",
      "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+\n",
      "file #1, shape=(9737, 3067), name=5c3c44b80379370013e0fd2b_train.csv\n",
      "mean position error 4.717837075733007\n",
      "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+\n",
      "process training set for site file num 2\n",
      "process testing set for site file num 2\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.12 GiB for an array with shape (7029, 21299) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b4f07314468b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_SPLITS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[0my_trainx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0my_trainy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1760\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1762\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1763\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   2078\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2080\u001b[1;33m             \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2082\u001b[0m             \u001b[1;31m# if the dim was reduced, then pass a lower-dim the next time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2118\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_slice_axis\u001b[1;34m(self, slice_obj, axis)\u001b[0m\n\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1750\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"iloc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_slice\u001b[1;34m(self, obj, axis, kind)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_slice\u001b[1;34m(self, slobj, axis, kind)\u001b[0m\n\u001b[0;32m   3612\u001b[0m         \"\"\"\n\u001b[0;32m   3613\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3614\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3615\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mget_slice\u001b[1;34m(self, slobj, axis)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m             \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice_take_blocks_ax0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[0m_slicer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_tuple)\u001b[0m\n\u001b[0;32m   1348\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                     blocks.append(\n\u001b[1;32m-> 1350\u001b[1;33m                         blk.take_nd(\n\u001b[0m\u001b[0;32m   1351\u001b[0m                             \u001b[0mblklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m                             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[0;32m   1294\u001b[0m             \u001b[0mallow_fill\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         new_values = algos.take_nd(\n\u001b[0m\u001b[0;32m   1297\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m   1655\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.12 GiB for an array with shape (7029, 21299) and data type int64"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Import libraries\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import psutil\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "# \n",
    "local_path = 'C:/Users/hp/Documents/OpenClassrooms/P8/'\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Fixed values\n",
    "# ------------------------------------------------------------------------------\n",
    "N_SPLITS = 10\n",
    "SEED = 42\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# File path definition\n",
    "# ------------------------------------------------------------------------------\n",
    "# LOG_PATH = Path(\"./log/\")\n",
    "LOG_PATH = Path(local_path + \"log/\")\n",
    "LOG_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Utilities\n",
    "# ------------------------------------------------------------------------------\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    t0 = time.time()\n",
    "    p = psutil.Process(os.getpid())\n",
    "    m0 = p.memory_info()[0] / 2. ** 30\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        m1 = p.memory_info()[0] / 2. ** 30\n",
    "        delta = m1 - m0\n",
    "        sign = '+' if delta >= 0 else '-'\n",
    "        delta = math.fabs(delta)\n",
    "        print(f\"[{m1:.1f}GB({sign}{delta:.1f}GB): {time.time() - t0:.3f}sec] {name}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "def comp_metric(xhat, yhat, fhat, x, y, f):\n",
    "    intermediate = np.sqrt(np.power(xhat-x, 2) + np.power(yhat-y, 2)) + 15 * np.abs(fhat-f)\n",
    "    return intermediate.sum()/xhat.shape[0]\n",
    "\n",
    "\n",
    "def score_log(df: pd.DataFrame, num_files: int, nam_file: str, data_shape: tuple, n_fold: int, seed: int, mpe: float):\n",
    "    score_dict = {'n_files': num_files, 'file_name': nam_file, 'shape': data_shape, 'fold': n_fold, 'seed': seed, 'score': mpe}\n",
    "    # noinspection PyTypeChecker\n",
    "    df = pd.concat([df, pd.DataFrame.from_dict([score_dict])])\n",
    "    df.to_csv(LOG_PATH / f\"log_score.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Set seed\n",
    "# ------------------------------------------------------------------------------\n",
    "set_seed(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Read data\n",
    "# ------------------------------------------------------------------------------\n",
    "# feature_dir = \"../input/indoor-navigation-and-location-wifi-features\"\n",
    "feature_dir = local_path  + \"input/indoor-navigation-and-location-wifi-features\"\n",
    "\n",
    "train_files = sorted(glob.glob(os.path.join(feature_dir, '*_train.csv')))\n",
    "print(\"There are {} *_train.csv files\".format(len(train_files)))\n",
    "test_files = sorted(glob.glob(os.path.join(feature_dir, '*_test.csv')))\n",
    "print(\"There are {} *_test.csv files\".format(len(test_files)))\n",
    "\n",
    "subm = pd.read_csv(local_path + 'input/submission/sample_submission.csv', index_col=0)\n",
    "# subm = pd.read_csv('../input/indoor-location-navigation/sample_submission.csv', index_col=0)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Define parameters for models\n",
    "# ------------------------------------------------------------------------------\n",
    "lgb_params = {'objective': 'root_mean_squared_error',\n",
    "              'boosting_type': 'gbdt',\n",
    "              'n_estimators': 50000,\n",
    "              'learning_rate': 0.1,\n",
    "              'num_leaves': 90,\n",
    "              'colsample_bytree': 0.4,\n",
    "              'subsample': 0.6,\n",
    "              'subsample_freq': 2,\n",
    "              'bagging_seed': SEED,\n",
    "              'reg_alpha': 8,\n",
    "              'reg_lambda': 2,\n",
    "              'random_state': SEED,\n",
    "              'n_jobs': -1\n",
    "              }\n",
    "# add settings for xgboost\n",
    "xgb_params = {'objective': 'reg:squarederror',\n",
    "              'booster': 'gbtree',\n",
    "              'n_estimators': 1000,\n",
    "              'learning_rate': 0.1,              \n",
    "              'reg_alpha': 8,\n",
    "              'reg_lambda': 2,\n",
    "              'random_state': SEED,\n",
    "              'n_jobs': -1\n",
    "              }\n",
    "\n",
    "lgb_f_params = {'objective': 'multiclass',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'n_estimators': 50000,\n",
    "                'learning_rate': 0.1,\n",
    "                'num_leaves': 90,\n",
    "                'colsample_bytree': 0.4,\n",
    "                'subsample': 0.6,\n",
    "                'subsample_freq': 2,\n",
    "                'bagging_seed': SEED,\n",
    "                'reg_alpha': 10,\n",
    "                'reg_lambda': 2,\n",
    "                'random_state': SEED,\n",
    "                'n_jobs': -1\n",
    "                }\n",
    "# WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "# UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
    "\n",
    "xgb_f_params = {'objective': 'multi:softmax', #multi:softmax  multi:softprob, will require use of encoder of type integer\n",
    "                'booster': 'gbtree',               \n",
    "                'n_estimators': 1000,\n",
    "                'learning_rate': 0.1,                \n",
    "                'reg_alpha': 10,\n",
    "                'reg_lambda': 2,\n",
    "                'random_state': SEED,                \n",
    "                'n_jobs': -1\n",
    "                }\n",
    "# ------------------------------------------------------------------------------\n",
    "# Training and inference\n",
    "# ------------------------------------------------------------------------------\n",
    "score_df = pd.DataFrame()\n",
    "oof = list()\n",
    "predictions = list()\n",
    "for n_files, file in enumerate(train_files):\n",
    "    # trick here is to use single \"for loop\" to process both train and test datasets\n",
    "    print(\"process training set for site file num {}\".format(n_files))\n",
    "    data = pd.read_csv(file, index_col=0)\n",
    "    print(\"process testing set for site file num {}\".format(n_files))\n",
    "    test_data = pd.read_csv(test_files[n_files], index_col=0)\n",
    "\n",
    "    oof_x, oof_y, oof_f = np.zeros(data.shape[0]), np.zeros(data.shape[0]), np.zeros(data.shape[0])\n",
    "    preds_x, preds_y = 0, 0\n",
    "    preds_f_arr = np.zeros((test_data.shape[0], N_SPLITS))\n",
    "\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(data.iloc[:, :-4])):\n",
    "        X_train = data.iloc[trn_idx, :-4]\n",
    "        y_trainx = data.iloc[trn_idx, -4]\n",
    "        y_trainy = data.iloc[trn_idx, -3]\n",
    "        y_trainf = data.iloc[trn_idx, -2]\n",
    "\n",
    "        X_valid = data.iloc[val_idx, :-4]\n",
    "        y_validx = data.iloc[val_idx, -4]\n",
    "        y_validy = data.iloc[val_idx, -3]\n",
    "        y_validf = data.iloc[val_idx, -2]\n",
    "\n",
    "        # modelx = lgb.LGBMRegressor(**lgb_params)\n",
    "        modelx = xgb.XGBRegressor(**xgb_params)\n",
    "        with timer(\"fit X\"):\n",
    "            modelx.fit(X_train, y_trainx,\n",
    "                       eval_set=[(X_valid, y_validx)],\n",
    "                       eval_metric='rmse',\n",
    "                       verbose=False,\n",
    "                       early_stopping_rounds=20\n",
    "                      )\n",
    "\n",
    "        # modely = lgb.LGBMRegressor(**lgb_params)\n",
    "        modely = xgb.XGBRegressor(**xgb_params)\n",
    "        with timer(\"fit Y\"):\n",
    "            modely.fit(X_train, y_trainy,\n",
    "                       eval_set=[(X_valid, y_validy)],\n",
    "                       eval_metric='rmse',\n",
    "                       verbose=False,\n",
    "                       early_stopping_rounds=20\n",
    "                       )\n",
    "\n",
    "        # modelf = lgb.LGBMClassifier(**lgb_f_params)\n",
    "        # remove param eval_metric='multi_logloss', for xgb\n",
    "        modelf = xgb.XGBClassifier(**xgb_f_params)\n",
    "        with timer(\"fit F\"):\n",
    "            modelf.fit(X_train, y_trainf,\n",
    "                       eval_set=[(X_valid, y_validf)],\n",
    "                       eval_metric='mlogloss', #for xgboot\n",
    "                       verbose=False,\n",
    "                       early_stopping_rounds=20\n",
    "                       )\n",
    "\n",
    "        oof_x[val_idx] = modelx.predict(X_valid)\n",
    "        oof_y[val_idx] = modely.predict(X_valid)\n",
    "        oof_f[val_idx] = modelf.predict(X_valid).astype(int)\n",
    "        \n",
    "        with timer(\"Pred X\"):\n",
    "            preds_x += modelx.predict(test_data.iloc[:, :-1]) / N_SPLITS\n",
    "            \n",
    "        with timer(\"Pred Y\"):\n",
    "            preds_y += modely.predict(test_data.iloc[:, :-1]) / N_SPLITS\n",
    "        \n",
    "        with timer(\"Pred F\"):\n",
    "            preds_f_arr[:, fold] = modelf.predict(test_data.iloc[:, :-1]).astype(int)\n",
    "\n",
    "        score = comp_metric(oof_x[val_idx], oof_y[val_idx], oof_f[val_idx],\n",
    "                            y_validx.to_numpy(), y_validy.to_numpy(), y_validf.to_numpy())\n",
    "        print(f\"fold {fold}: mean position error {score}\")\n",
    "        score_df = score_log(score_df, n_files, os.path.basename(file), data.shape, fold, SEED, score)\n",
    "\n",
    "    print(\"*+\"*40)\n",
    "    print(f\"file #{n_files}, shape={data.shape}, name={os.path.basename(file)}\")\n",
    "    score = comp_metric(oof_x, oof_y, oof_f,\n",
    "                        data.iloc[:, -4].to_numpy(), data.iloc[:, -3].to_numpy(), data.iloc[:, -2].to_numpy())\n",
    "    oof.append(score)\n",
    "    print(f\"mean position error {score}\")\n",
    "    print(\"*+\"*40)\n",
    "    score_df = score_log(score_df, n_files, os.path.basename(file), data.shape, 999, SEED, score)\n",
    "\n",
    "    preds_f_mode = stats.mode(preds_f_arr, axis=1)\n",
    "    preds_f = preds_f_mode[0].astype(int).reshape(-1)\n",
    "    test_preds = pd.DataFrame(np.stack((preds_f, preds_x, preds_y))).T\n",
    "    test_preds.columns = subm.columns\n",
    "    test_preds.index = test_data[\"site_path_timestamp\"]\n",
    "    test_preds[\"floor\"] = test_preds[\"floor\"].astype(int)\n",
    "    predictions.append(test_preds)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Submit the result\n",
    "# ------------------------------------------------------------------------------\n",
    "all_preds = pd.concat(predictions)\n",
    "all_preds = all_preds.reindex(subm.index)\n",
    "all_preds.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24 *_train.csv files\n",
      "There are 24 *_test.csv files\n",
      "process training set for site file num 0\n",
      "process testing set for site file num 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0.8GB(-0.0GB): 1875.343sec] fit X\n",
      "[0.8GB(+0.0GB): 2011.414sec] fit Y\n",
      "[1.0GB(+0.1GB): 64.917sec] fit F\n",
      "[1.2GB(+0.0GB): 17.939sec] Pred X\n",
      "[1.2GB(+0.0GB): 17.843sec] Pred Y\n",
      "[1.2GB(+0.0GB): 3.309sec] Pred F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0: mean position error 63.214866311881686\n",
      "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+\n",
      "file #0, shape=(9296, 3401), name=5a0546857ecc773753327266_train.csv\n",
      "mean position error 155.8233074427557\n",
      "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+\n"
     ]
    }
   ],
   "source": [
    "# for SVM and SVR\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Import libraries\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "import psutil\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "# \n",
    "local_path = 'C:/Users/hp/Documents/OpenClassrooms/P8/'\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Fixed values\n",
    "# ------------------------------------------------------------------------------\n",
    "N_SPLITS = 10\n",
    "SEED = 42\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# File path definition\n",
    "# ------------------------------------------------------------------------------\n",
    "# LOG_PATH = Path(\"./log/\")\n",
    "LOG_PATH = Path(local_path + \"log/\")\n",
    "LOG_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Utilities\n",
    "# ------------------------------------------------------------------------------\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    t0 = time.time()\n",
    "    p = psutil.Process(os.getpid())\n",
    "    m0 = p.memory_info()[0] / 2. ** 30\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        m1 = p.memory_info()[0] / 2. ** 30\n",
    "        delta = m1 - m0\n",
    "        sign = '+' if delta >= 0 else '-'\n",
    "        delta = math.fabs(delta)\n",
    "        print(f\"[{m1:.1f}GB({sign}{delta:.1f}GB): {time.time() - t0:.3f}sec] {name}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "def comp_metric(xhat, yhat, fhat, x, y, f):\n",
    "    intermediate = np.sqrt(np.power(xhat-x, 2) + np.power(yhat-y, 2)) + 15 * np.abs(fhat-f)\n",
    "    return intermediate.sum()/xhat.shape[0]\n",
    "\n",
    "\n",
    "def score_log(df: pd.DataFrame, num_files: int, nam_file: str, data_shape: tuple, n_fold: int, seed: int, mpe: float):\n",
    "    score_dict = {'n_files': num_files, 'file_name': nam_file, 'shape': data_shape, 'fold': n_fold, 'seed': seed, 'score': mpe}\n",
    "    # noinspection PyTypeChecker\n",
    "    df = pd.concat([df, pd.DataFrame.from_dict([score_dict])])\n",
    "    df.to_csv(LOG_PATH / f\"log_score.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Set seed\n",
    "# ------------------------------------------------------------------------------\n",
    "set_seed(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Read data\n",
    "# ------------------------------------------------------------------------------\n",
    "# feature_dir = \"../input/indoor-navigation-and-location-wifi-features\"\n",
    "feature_dir = local_path  + \"input/indoor-navigation-and-location-wifi-features\"\n",
    "\n",
    "train_files = sorted(glob.glob(os.path.join(feature_dir, '*_train.csv')))\n",
    "print(\"There are {} *_train.csv files\".format(len(train_files)))\n",
    "test_files = sorted(glob.glob(os.path.join(feature_dir, '*_test.csv')))\n",
    "print(\"There are {} *_test.csv files\".format(len(test_files)))\n",
    "\n",
    "subm = pd.read_csv(local_path + 'input/submission/sample_submission.csv', index_col=0)\n",
    "# subm = pd.read_csv('../input/indoor-location-navigation/sample_submission.csv', index_col=0)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Define parameters for models\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# add settings for xgboost\n",
    "xgb_params = {'objective': 'reg:squarederror',\n",
    "              'booster': 'gbtree',\n",
    "              'n_estimators': 1000,\n",
    "              'learning_rate': 0.1,              \n",
    "              'reg_alpha': 8,\n",
    "              'reg_lambda': 2,\n",
    "              'random_state': SEED,\n",
    "              'n_jobs': -1\n",
    "              }\n",
    "\n",
    "\n",
    "# WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "# UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
    "\n",
    "xgb_f_params = {'objective': 'multi:softmax', #multi:softmax  multi:softprob, will require use of encoder of type integer\n",
    "                'booster': 'gbtree',               \n",
    "                'n_estimators': 1000,\n",
    "                'learning_rate': 0.1,                \n",
    "                'reg_alpha': 10,\n",
    "                'reg_lambda': 2,\n",
    "                'random_state': SEED,                \n",
    "                'n_jobs': -1\n",
    "                }\n",
    "# ------------------------------------------------------------------------------\n",
    "# Training and inference\n",
    "# ------------------------------------------------------------------------------\n",
    "score_df = pd.DataFrame()\n",
    "oof = list()\n",
    "predictions = list()\n",
    "for n_files, file in enumerate(train_files):\n",
    "    # trick here is to use single \"for loop\" to process both train and test datasets\n",
    "    print(\"process training set for site file num {}\".format(n_files))\n",
    "    data = pd.read_csv(file, index_col=0)\n",
    "    print(\"process testing set for site file num {}\".format(n_files))\n",
    "    test_data = pd.read_csv(test_files[n_files], index_col=0)\n",
    "\n",
    "    oof_x, oof_y, oof_f = np.zeros(data.shape[0]), np.zeros(data.shape[0]), np.zeros(data.shape[0])\n",
    "    preds_x, preds_y = 0, 0\n",
    "    preds_f_arr = np.zeros((test_data.shape[0], N_SPLITS))\n",
    "\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(data.iloc[:, :-4])):\n",
    "        X_train = data.iloc[trn_idx, :-4]\n",
    "        y_trainx = data.iloc[trn_idx, -4]\n",
    "        y_trainy = data.iloc[trn_idx, -3]\n",
    "        y_trainf = data.iloc[trn_idx, -2]\n",
    "\n",
    "        X_valid = data.iloc[val_idx, :-4]\n",
    "        y_validx = data.iloc[val_idx, -4]\n",
    "        y_validy = data.iloc[val_idx, -3]\n",
    "        y_validf = data.iloc[val_idx, -2]\n",
    "\n",
    "        # modelx = lgb.LGBMRegressor(**lgb_params)\n",
    "        # modelx = xgb.XGBRegressor(**xgb_params)\n",
    "        modelx = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "        # modelx = SVR(kernel='linear', C=100, gamma='auto')\n",
    "        # modelx = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1)\n",
    "        with timer(\"fit X\"):\n",
    "            modelx.fit(X_train, y_trainx,\n",
    "                       # eval_set=[(X_valid, y_validx)],\n",
    "                       # eval_metric='rmse',\n",
    "                       #verbose=False,\n",
    "                       #early_stopping_rounds=20\n",
    "                      )\n",
    "\n",
    "        # modely = lgb.LGBMRegressor(**lgb_params)\n",
    "        # modely = xgb.XGBRegressor(**xgb_params)\n",
    "        modely = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "        # modely = SVR(kernel='linear', C=100, gamma='auto')\n",
    "        # modely = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1)\n",
    "        with timer(\"fit Y\"):\n",
    "            modely.fit(X_train, y_trainy #,\n",
    "                       # eval_set=[(X_valid, y_validy)],\n",
    "                       # eval_metric='rmse',\n",
    "                       # verbose=False,\n",
    "                       # early_stopping_rounds=20\n",
    "                       )\n",
    "\n",
    "        # modelf = lgb.LGBMClassifier(**lgb_f_params)\n",
    "        # remove param eval_metric='multi_logloss', for xgb\n",
    "        # modelf = xgb.XGBClassifier(**xgb_f_params)\n",
    "        modelf = SVC(decision_function_shape='ovo')\n",
    "        with timer(\"fit F\"):\n",
    "            modelf.fit(X_train, y_trainf #,\n",
    "                       # eval_set=[(X_valid, y_validf)],\n",
    "                       # eval_metric='mlogloss', #for xgboot\n",
    "                       # verbose=False,\n",
    "                       # early_stopping_rounds=20\n",
    "                       )\n",
    "\n",
    "        oof_x[val_idx] = modelx.predict(X_valid)\n",
    "        oof_y[val_idx] = modely.predict(X_valid)\n",
    "        oof_f[val_idx] = modelf.predict(X_valid).astype(int)\n",
    "        \n",
    "        with timer(\"Pred X\"):\n",
    "            preds_x += modelx.predict(test_data.iloc[:, :-1]) / N_SPLITS\n",
    "            \n",
    "        with timer(\"Pred Y\"):\n",
    "            preds_y += modely.predict(test_data.iloc[:, :-1]) / N_SPLITS\n",
    "        \n",
    "        with timer(\"Pred F\"):\n",
    "            preds_f_arr[:, fold] = modelf.predict(test_data.iloc[:, :-1]).astype(int)\n",
    "\n",
    "        score = comp_metric(oof_x[val_idx], oof_y[val_idx], oof_f[val_idx],\n",
    "                            y_validx.to_numpy(), y_validy.to_numpy(), y_validf.to_numpy())\n",
    "        print(f\"fold {fold}: mean position error {score}\")\n",
    "        score_df = score_log(score_df, n_files, os.path.basename(file), data.shape, fold, SEED, score)\n",
    "        \n",
    "        break # for test purpose otherwise remove this break statement\n",
    "\n",
    "    print(\"*+\"*40)\n",
    "    print(f\"file #{n_files}, shape={data.shape}, name={os.path.basename(file)}\")\n",
    "    score = comp_metric(oof_x, oof_y, oof_f,\n",
    "                        data.iloc[:, -4].to_numpy(), data.iloc[:, -3].to_numpy(), data.iloc[:, -2].to_numpy())\n",
    "    oof.append(score)\n",
    "    print(f\"mean position error {score}\")\n",
    "    print(\"*+\"*40)\n",
    "    score_df = score_log(score_df, n_files, os.path.basename(file), data.shape, 999, SEED, score)\n",
    "\n",
    "    preds_f_mode = stats.mode(preds_f_arr, axis=1)\n",
    "    preds_f = preds_f_mode[0].astype(int).reshape(-1)\n",
    "    test_preds = pd.DataFrame(np.stack((preds_f, preds_x, preds_y))).T\n",
    "    test_preds.columns = subm.columns\n",
    "    test_preds.index = test_data[\"site_path_timestamp\"]\n",
    "    test_preds[\"floor\"] = test_preds[\"floor\"].astype(int)\n",
    "    predictions.append(test_preds)\n",
    "    break # for test purpose otherwise remove this break statement\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Submit the result\n",
    "# ------------------------------------------------------------------------------\n",
    "all_preds = pd.concat(predictions)\n",
    "all_preds = all_preds.reindex(subm.index)\n",
    "all_preds.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
